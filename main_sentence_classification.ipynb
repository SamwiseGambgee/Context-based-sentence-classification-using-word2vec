{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Based Sentence Classification\n",
    "\n",
    "### This program classifies sentences based on the context and predicts whether a sentence might be related to a \"patient\" or \"doctor\" spoken sentences.\n",
    "\n",
    "\n",
    "<strong> Run the cell below to import all the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Anacondas\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing all the required modules and the helper functions \n",
    "\n",
    "import numpy as np \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import pickle \n",
    "\n",
    "# the following two modules are helper functions to generate features from the sentences\n",
    "def get_feature(text,feature_dimension,wordset,model, label = None):\n",
    "    features = None\n",
    "    \n",
    "    for sample in text:\n",
    "        paragraph = sample.lower()\n",
    "        sentences = sent_tokenize(paragraph)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            feature_vector = np.zeros(feature_dimension)\n",
    "            words = word_tokenize(sentence)\n",
    "\n",
    "            count = 0\n",
    "            for word in words:\n",
    "                if word in wordset and word.isalnum():\n",
    "                    count = count + 1\n",
    "                    feature_vector = feature_vector + model[word]\n",
    "\n",
    "            if count != 0:\n",
    "                feature_vector = feature_vector / float(count)\n",
    "\n",
    "                if label is not None:\n",
    "                    feature_vector = np.append(feature_vector, label)\n",
    "\n",
    "                feature_vector = feature_vector[np.newaxis]\n",
    "\n",
    "                if features is None:\n",
    "                    features = feature_vector\n",
    "                else:\n",
    "                    features = np.concatenate((features, feature_vector))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def generate_features(feature_dimension,wordset,model):\n",
    "\n",
    "    with open(\"patient.txt\") as patfile:\n",
    "        patient = patfile.readlines()\n",
    "    patfile.close()\n",
    "\n",
    "    with open(\"doctor.txt\") as docfile:\n",
    "        doctor = docfile.readlines()\n",
    "    docfile.close()\n",
    "\n",
    "    patient_features = get_feature(patient,feature_dimension,wordset,model,label=0)\n",
    "    doctor_features = get_feature(doctor,feature_dimension,wordset,model,label=1)\n",
    "\n",
    "    features = np.concatenate((patient_features,doctor_features))\n",
    "    return features\n",
    "\n",
    "def predict(clf, text,feature_dimension, wordset, model):\n",
    "    paragraph = text.lower()\n",
    "    sentences = sent_tokenize(paragraph)   \n",
    "    \n",
    "    features = get_feature([text],feature_dimension,wordset,model)\n",
    "    \n",
    "    pred = clf.predict(features)\n",
    "                       \n",
    "    for i,item in enumerate(pred):\n",
    "        if item == 0:\n",
    "            ret = \"patient\"\n",
    "        else:\n",
    "            ret = \"doctor\"\n",
    "\n",
    "        print(\"{} : {}\".format(sentences[i],ret))\n",
    "    \n",
    "    print()\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection \n",
    "\n",
    "The data is crawled from www.askthedoctor.com.\n",
    "The website contains data of questions asked by the patients, and the corresponding answers given by the doctor.\n",
    "The data is categorized into different categories based on the diseases. Here, each of the category is looped and corresponding data is stored as \"patient\" data or \"doctor\" data\n",
    "Run the code below to collect data from the above website. \n",
    "Two files \"patient.txt\" and \"doctor.txt\" are saved\n",
    "\n",
    "<strong>Note that it might take several minutes depending on the internet connection. \n",
    "\n",
    "<strong>Skip the below cell if the data is already saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base_url = \"https://www.askthedoctor.com/browse-medical-questions\"\n",
    "base_f = urllib.request.urlopen(base_url)\n",
    "base_soup = BeautifulSoup(base_f,\"lxml\")\n",
    "\n",
    "# categories of diseases \n",
    "categories = [(base_anchor[\"href\"],base_anchor[\"title\"]) for base_div in base_soup.findAll(\"div\",{\"class\":\"disease_column\"}) for base_anchor in base_div.findAll(\"a\",{\"itemtype\":\"https://schema.org/category\"})]\n",
    "\n",
    "\n",
    "print(\"Collecting data ... \")\n",
    "\n",
    "with open(\"patient.txt\",\"w\") as patientfile, open(\"doctor.txt\", \"w\") as doctorfile:\n",
    "    for category in categories:\n",
    "\n",
    "        topic = category[1]\n",
    "        print(topic)\n",
    "\n",
    "        try:\n",
    "            url = category[0]\n",
    "            f = urllib.request.urlopen(url)\n",
    "            soup = BeautifulSoup(f,\"lxml\")\n",
    "\n",
    "            divs = soup.findAll('div',{\"class\":\"question_az\"})\n",
    "\n",
    "            for i,div in enumerate(divs):\n",
    "                inner_url = div.find('a')['href']\n",
    "                inner_f = urllib.request.urlopen(inner_url)\n",
    "                inner_soup = BeautifulSoup(inner_f,\"lxml\")\n",
    "\n",
    "                question = inner_soup.find('span',{\"class\":\"quesans\"})\n",
    "                question = question.text.replace(\",\",\" \")\n",
    "                question = re.sub('[.]+', '.',question)\n",
    "\n",
    "\n",
    "                for token in sent_tokenize(question):\n",
    "                    if len(word_tokenize(token)) > 3:\n",
    "                        patientfile.write(\"{}\\n\".format(token))\n",
    "\n",
    "                answer = inner_soup.find('span', {\"class\": \"answer quesans\"})\n",
    "                answer = answer.text.replace(\"\"\" \\n(adsbygoogle = window.adsbygoogle || []).push({});\"\"\",\"\").replace(\"\\n\",\" \").replace(\"   \",\" \").replace(\",\",\" \")\n",
    "                answer = re.sub('[.]+', '.',answer)\n",
    "\n",
    "                for token in sent_tokenize(answer):\n",
    "                    if len(word_tokenize(token)) > 3:\n",
    "                        doctorfile.write(\"{}\\n\".format(token))\n",
    "\n",
    "        except:\n",
    "            print(\"Error ................ {}\".format(topic))\n",
    "            \n",
    "patientfile.close()\n",
    "doctorfile.close()\n",
    "\n",
    "print(\"Data saved !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2vec is a method of word embeddings where the words in a sentence are mapped to their corresponding vectors representation.\n",
    "Here the whole dataset is considered, both patient and doctor spoken sentences to learn word embeddings.\n",
    "\n",
    "A python library \"gensim\" is used to train a word2vec model\n",
    "\n",
    "<strong> Run the code below to generate a word2vec model. \n",
    "<string> Skip the cell below if model is already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset consists of 25601 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-16 12:37:47,969 : INFO : collecting all words and their counts\n",
      "2017-09-16 12:37:47,973 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-16 12:37:48,139 : INFO : PROGRESS: at sentence #10000, processed 149033 words, keeping 9961 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-16 12:37:48,270 : INFO : PROGRESS: at sentence #20000, processed 294765 words, keeping 13963 word types\n",
      "2017-09-16 12:37:48,330 : INFO : collected 15461 word types from a corpus of 375367 raw words and 25415 sentences\n",
      "2017-09-16 12:37:48,335 : INFO : Loading a fresh vocabulary\n",
      "2017-09-16 12:37:48,403 : INFO : min_count=5 retains 4644 unique words (30% of original 15461, drops 10817)\n",
      "2017-09-16 12:37:48,406 : INFO : min_count=5 leaves 357715 word corpus (95% of original 375367, drops 17652)\n",
      "2017-09-16 12:37:48,470 : INFO : deleting the raw counts dictionary of 15461 items\n",
      "2017-09-16 12:37:48,477 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2017-09-16 12:37:48,481 : INFO : downsampling leaves estimated 262582 word corpus (73.4% of prior 357715)\n",
      "2017-09-16 12:37:48,494 : INFO : estimated required memory for 4644 words and 300 dimensions: 13467600 bytes\n",
      "2017-09-16 12:37:48,554 : INFO : resetting layer weights\n",
      "2017-09-16 12:37:48,797 : INFO : training model with 4 workers on 4644 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-09-16 12:37:49,845 : INFO : PROGRESS: at 21.84% examples, 278212 words/s, in_qsize 8, out_qsize 0\n",
      "2017-09-16 12:37:50,859 : INFO : PROGRESS: at 51.70% examples, 331869 words/s, in_qsize 7, out_qsize 0\n",
      "2017-09-16 12:37:51,880 : INFO : PROGRESS: at 87.76% examples, 376404 words/s, in_qsize 8, out_qsize 0\n",
      "2017-09-16 12:37:52,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-16 12:37:52,141 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-16 12:37:52,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-16 12:37:52,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-16 12:37:52,169 : INFO : training on 1876835 raw words (1312829 effective words) took 3.4s, 391576 effective words/s\n",
      "2017-09-16 12:37:52,171 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-09-16 12:37:52,231 : INFO : saving Word2Vec object under word2vec_model, separately None\n",
      "2017-09-16 12:37:52,233 : INFO : not storing attribute syn0norm\n",
      "2017-09-16 12:37:52,235 : INFO : not storing attribute cum_table\n",
      "2017-09-16 12:37:52,391 : INFO : saved word2vec_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved !\n"
     ]
    }
   ],
   "source": [
    "data_matrix = [] \n",
    "\n",
    "with open(\"patient.txt\",\"r\") as patfile, open(\"doctor.txt\",\"r\") as docfile:\n",
    "    data_matrix = patfile.readlines()\n",
    "    data_matrix.extend(docfile.readlines())\n",
    "    \n",
    "patfile.close()\n",
    "docfile.close()\n",
    "\n",
    "# converting the whole data into lower case\n",
    "data_matrix = [sample.lower() for sample in data_matrix]\n",
    "\n",
    "print(\"The Dataset consists of {} sentences.\".format(len(data_matrix)))\n",
    "\n",
    "# Formatting the data to provide as input to gensim package's word2vec model\n",
    "words_matrix = []\n",
    "for sample in data_matrix:\n",
    "    sentences = sent_tokenize(sample)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words_new = [word for word in words if word.isalnum()]\n",
    "        words_matrix.append(words_new)\n",
    "        \n",
    "        \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Parameters required for training word2vec model\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 5   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(words_matrix, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"word2vec_model\"\n",
    "model.save(model_name)\n",
    "\n",
    "print(\"Word2Vec model saved !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Word2Vec Model \n",
    "\n",
    "Given a word, the model should be able to give similar words after being trained depending on the context words that appeared in sentences of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-16 12:37:59,313 : INFO : loading Word2Vec object from word2vec_model\n",
      "2017-09-16 12:37:59,734 : INFO : loading wv recursively from word2vec_model.wv.* with mmap=None\n",
      "2017-09-16 12:37:59,737 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-09-16 12:37:59,744 : INFO : setting ignored attribute cum_table to None\n",
      "2017-09-16 12:37:59,749 : INFO : loaded word2vec_model\n",
      "2017-09-16 12:37:59,800 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4644 words in the vocabulary and the dimension of the vectors is 300\n",
      "I\n",
      "[('addressed', 0.6825687885284424), ('you', 0.67529296875), ('about', 0.6483484506607056), ('appointments', 0.6423941850662231), ('party', 0.6292402744293213), ('answered', 0.6146000027656555), ('anxious', 0.6127475500106812), ('helped', 0.6091926097869873), ('efforts', 0.6089515686035156), ('now', 0.5993724465370178)]\n",
      "\n",
      "swelling\n",
      "[('inflammation', 0.9732186198234558), ('ligaments', 0.9714838266372681), ('node', 0.9705648422241211), ('lymph', 0.9691843390464783), ('behind', 0.9660965204238892), ('itching', 0.9660079479217529), ('weakness', 0.9643855690956116), ('lymphedema', 0.9634043574333191), ('discomfort', 0.9624897241592407), ('heart', 0.9600017666816711)]\n",
      "\n",
      "headache\n",
      "[('vomiting', 0.988715410232544), ('muscle', 0.98659348487854), ('mild', 0.9836856126785278), ('severe', 0.9831716418266296), ('drowsiness', 0.9827138781547546), ('burning', 0.9813575148582458), ('pains', 0.9811801910400391), ('weakness', 0.9786884784698486), ('numbness', 0.9769753813743591), ('sensation', 0.9768819808959961)]\n",
      "\n",
      "fever\n",
      "[('vomiting', 0.9707344770431519), ('headache', 0.9615980982780457), ('nipple', 0.9604698419570923), ('nausea', 0.9597859382629395), ('mild', 0.9584822058677673), ('injury', 0.9578261375427246), ('itching', 0.9571393728256226), ('less', 0.954887866973877), ('bleeding', 0.9547781944274902), ('passing', 0.9525362253189087)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec_model\")\n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "\n",
    "print(\"The model has {} words in the vocabulary and the dimension of the vectors is {}\".format(word_vectors.shape[0],word_vectors.shape[1]))\n",
    "\n",
    "\n",
    "print(\"I\\n{}\\n\".format(model.most_similar(\"i\")))\n",
    "print(\"swelling\\n{}\\n\".format(model.most_similar(\"swelling\")))\n",
    "print(\"headache\\n{}\\n\".format(model.most_similar(\"headache\")))\n",
    "print(\"fever\\n{}\\n\".format(model.most_similar(\"fever\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the features\n",
    "\n",
    "Each sentence vector is formed by averaging the vectors corresponding to each word in a sentence. \n",
    "\n",
    "The whole set of features is divided into train and test features.\n",
    "\n",
    "<strong> This might take some time\n",
    "<strong> Skip the cell if features are already saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-16 12:38:12,414 : INFO : loading Word2Vec object from word2vec_model\n",
      "2017-09-16 12:38:12,768 : INFO : loading wv recursively from word2vec_model.wv.* with mmap=None\n",
      "2017-09-16 12:38:12,770 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-09-16 12:38:12,774 : INFO : setting ignored attribute cum_table to None\n",
      "2017-09-16 12:38:12,778 : INFO : loaded word2vec_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features ...\n",
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec_model\")\n",
    "word_vectors = model.wv.syn0\n",
    "\n",
    "feature_dimension = word_vectors.shape[1]\n",
    "\n",
    "# all words in the vocabulary\n",
    "wordset = set(model.wv.index2word)\n",
    "\n",
    "\n",
    "print(\"Generating features ...\")\n",
    "features = generate_features(feature_dimension,wordset,model)\n",
    "\n",
    "# dividing the dataset into train and test datasets.\n",
    "indices = np.random.permutation(features.shape[0])\n",
    "test_idx,training_idx = indices[:2000], indices[2000:]\n",
    "test_features, train_features = features[test_idx,:], features[training_idx,:]\n",
    "\n",
    "train_labels =  train_features[:,-1]\n",
    "train_features = train_features[:,:-1]\n",
    "\n",
    "test_labels =  test_features[:,-1]\n",
    "test_features = test_features[:,:-1]\n",
    "\n",
    "# Saving features \n",
    "np.save(\"train_features.npy\",train_features)\n",
    "np.save(\"train_labels.npy\", train_labels)\n",
    "np.save(\"test_features.npy\",test_features)\n",
    "np.save(\"test_labels.npy\",test_labels)\n",
    "\n",
    "print(\"Features saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classifier Model\n",
    "\n",
    "A simple SVM classifier is trained by converting a sentence into vectors.\n",
    "\n",
    "<strong> Run the following code a train a simple SVM classifier and save the model. This might take some time\n",
    "<Strong> Skip the cell if model is already generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier ....\n",
      "Classifier Model saved !\n"
     ]
    }
   ],
   "source": [
    "# Loading train features\n",
    "train_features = np.load(\"train_features.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\",C=100)\n",
    "\n",
    "print(\"Training classifier ....\")\n",
    "clf.fit(train_features,train_labels)\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open(\"clf_model.pkl\",\"wb\") as clffile:\n",
    "    pickle.dump(clf,clffile)\n",
    "    \n",
    "clffile.close()\n",
    "\n",
    "print(\"Classifier Model saved !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for 2000 samples for the model : 86.1%\n"
     ]
    }
   ],
   "source": [
    "with open(\"clf_model.pkl\",\"rb\") as clffile:\n",
    "    clf = pickle.load(clffile)\n",
    "    \n",
    "clffile.close()\n",
    "   \n",
    "test_features = np.load(\"test_features.npy\")\n",
    "test_labels = np.load(\"test_labels.npy\")\n",
    "\n",
    "\n",
    "pred = clf.predict(test_features)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(test_labels,pred)*100\n",
    "\n",
    "print(\"The accuracy for {} samples for the model : {}%\".format(test_features.shape[0],acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out some random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-16 12:53:55,008 : INFO : loading Word2Vec object from word2vec_model\n",
      "2017-09-16 12:53:55,306 : INFO : loading wv recursively from word2vec_model.wv.* with mmap=None\n",
      "2017-09-16 12:53:55,309 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-09-16 12:53:55,312 : INFO : setting ignored attribute cum_table to None\n",
      "2017-09-16 12:53:55,318 : INFO : loaded word2vec_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i still cough few times a day. : patient\n",
      "what should i do? : patient\n",
      "\n",
      "i have severe pain in my abdomen. : patient\n",
      "do i have to go to the doctor? : patient\n",
      "wash your hands everytime and follow hygenic practices : doctor\n",
      "\n",
      "i have a sore throat. : patient\n",
      "it has been there for the past week. : patient\n",
      "\n",
      "do you have sore throat? : doctor\n",
      "does your throat feel itchy? : doctor\n",
      "do you have flu? : doctor\n",
      "\n",
      "you should apply neomycin ointment on your chin : doctor\n",
      "\n",
      "do you think i have infection which is causing my blood pressure to rise? : patient\n",
      "yes, your blood pressure is increasing because of infection : doctor\n",
      "\n",
      "are you comfortable? : doctor\n",
      "if you are not comfortable, please let me know. : doctor\n",
      "no i am not comfortable and in too much in pain right now. : patient\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"clf_model.pkl\",\"rb\") as clffile:\n",
    "    clf = pickle.load(clffile)\n",
    "\n",
    "clffile.close()\n",
    "\n",
    "model = Word2Vec.load(\"word2vec_model\")\n",
    "word_vectors = model.wv.syn0\n",
    "\n",
    "feature_dimension = word_vectors.shape[1]\n",
    "\n",
    "# all words in the vocabulary\n",
    "wordset = set(model.wv.index2word)\n",
    "\n",
    "\n",
    "text = \"i still cough few times a day. what should i do?\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n",
    "\n",
    "text = \"i have severe pain in my abdomen. do i have to go to the doctor? wash your hands everytime and follow hygenic practices\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n",
    "\n",
    "text = \"i have a sore throat. it has been there for the past week.\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n",
    "\n",
    "text = \"do you have sore throat? Does your throat feel itchy? Do you have flu?\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n",
    "\n",
    "\n",
    "text = \"you should apply neomycin ointment on your chin\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n",
    "\n",
    "\n",
    "text = \"do you think I have infection which is causing my blood pressure to rise? Yes, your blood pressure is increasing because of infection\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n",
    "\n",
    "text = \"Are you comfortable? If you are not comfortable, please let me know. No I am not comfortable and in too much in pain right now.\"\n",
    "res = predict(clf, text,feature_dimension, wordset, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
